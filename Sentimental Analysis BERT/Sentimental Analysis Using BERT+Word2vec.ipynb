{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663049cf",
   "metadata": {
    "id": "663049cf"
   },
   "source": [
    "## Task 2\n",
    "> 1. Implement multi class text classification on your choice dataset using open-source language model like BERT.\n",
    ">2. Also train the same data using word embeddings, compare both the models and choose the best one.\n",
    ">3. Document your views on the data, Suggest few alternate solutions for the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ba9c2",
   "metadata": {
    "id": "879ba9c2"
   },
   "source": [
    "\n",
    "\n",
    "**BBC News Articles Dataset**\n",
    "\n",
    "The \"BBC News Articles Dataset\" is a collection of textual data comprising 2,225 individual documents originally published on the BBC news website. These documents encompass news stories from a range of topical areas, of the period from 2004 to 2005.\n",
    "\n",
    "**Key Attributes:**\n",
    "- **Total Documents:** 2,225\n",
    "- **Class Labels:** 5\n",
    "- **Class Labels:** business, entertainment, politics, sport, and tech\n",
    "- **Copyright:** All rights, including copyright, in the content of the original articles are owned by the BBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6267967b",
   "metadata": {
    "id": "6267967b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010ba2a9",
   "metadata": {
    "id": "010ba2a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                               Text\n",
       "0  business  Ad sales boost Time Warner profit\\n\\nQuarterly...\n",
       "1  business  Dollar gains on Greenspan speech\\n\\nThe dollar...\n",
       "2  business  Yukos unit buyer faces loan claim\\n\\nThe owner...\n",
       "3  business  High fuel prices hit BA's profits\\n\\nBritish A...\n",
       "4  business  Pernod takeover talk lifts Domecq\\n\\nShares in..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "bbc_folder = os.path.join(current_directory, 'bbc')\n",
    "\n",
    "text_data = []\n",
    "\n",
    "categories = [\"business\", \"entertainment\", \"politics\", \"sport\", \"tech\"]\n",
    "\n",
    "for category in categories:\n",
    "    category_folder = os.path.join(bbc_folder, category)\n",
    "\n",
    "\n",
    "    for filename in os.listdir(category_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(category_folder, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                text_data.append({'Category': category, 'Text': content})\n",
    "\n",
    "df = pd.DataFrame(text_data)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7e9cb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_bbc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb316f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_bbc_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3444649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1690ad06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1690ad06",
    "outputId": "344aca7f-1f86-4596-d66a-b5ddade8c415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Category  2225 non-null   object\n",
      " 1   Text      2225 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c43051",
   "metadata": {
    "id": "39c43051"
   },
   "source": [
    "Now we have loaded all the files the bbc folder has"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3b8d1",
   "metadata": {
    "id": "cfc3b8d1"
   },
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab1c62",
   "metadata": {
    "id": "dbab1c62"
   },
   "source": [
    "Now we'll pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0d1403",
   "metadata": {
    "id": "1d0d1403"
   },
   "source": [
    "Removing New line break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "250f76a6",
   "metadata": {
    "id": "250f76a6"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'\\n', ' ', text)\n",
    "    return cleaned_text\n",
    "\n",
    "df['Text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf85eb",
   "metadata": {
    "id": "6fbf85eb"
   },
   "source": [
    "Removing any special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f3d57f",
   "metadata": {
    "id": "70f3d57f"
   },
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64850a54",
   "metadata": {
    "id": "64850a54"
   },
   "source": [
    "as digits and no are plays a vital role in news, i am not removing them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01440e",
   "metadata": {
    "id": "1a01440e"
   },
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a517f3",
   "metadata": {
    "id": "b4a517f3"
   },
   "source": [
    "I have combined nltk stop words and bbc stop words to curate a custom stop words will I'll use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04275723",
   "metadata": {
    "id": "04275723"
   },
   "outputs": [],
   "source": [
    "custom_stopwords = list(set([\n",
    "    'a', 'about', 'above', 'according', 'across', 'actually', 'adj', 'after', 'afterwards', 'again', 'all', 'almost',\n",
    "    'along', 'already', 'also', 'although', 'always', 'among', 'amongst', 'an', 'am', 'and', 'another', 'any', 'anyhow',\n",
    "    'anyone', 'anything', 'anywhere', 'are', 'aren', 'aren\\'t', 'around', 'as', 'at', 'be', 'became', 'because', 'become',\n",
    "    'becomes', 'been', 'beforehand', 'begin', 'being', 'below', 'beside', 'besides', 'between', 'both', 'but', 'by', 'can',\n",
    "    'cannot', 'can\\'t', 'caption', 'co', 'come', 'could', 'couldn', 'couldn\\'t', 'did', 'didn', 'didn\\'t', 'do', 'does',\n",
    "    'doesn', 'doesn\\'t', 'don', 'don\\'t', 'down', 'during', 'each', 'early', 'eg', 'either', 'else', 'elsewhere', 'end',\n",
    "    'ending', 'enough', 'etc', 'even', 'ever', 'every', 'everywhere', 'except', 'few', 'for', 'found', 'from', 'further',\n",
    "    'had', 'has', 'hasn', 'hasn\\'t', 'have', 'haven', 'haven\\'t', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby',\n",
    "    'herein', 'hereupon', 'hers', 'him', 'his', 'how', 'however', 'ie', 'i.e.', 'if', 'in', 'inc', 'inc.', 'indeed', 'instead',\n",
    "    'into', 'is', 'isn', 'isn\\'t', 'it', 'its', 'itself', 'last', 'late', 'later', 'less', 'let', 'like', 'likely', 'll',\n",
    "    'ltd', 'made', 'make', 'makes', 'many', 'may', 'maybe', 'me', 'meantime', 'meanwhile', 'might', 'miss', 'more', 'most',\n",
    "    'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'namely', 'near', 'neither', 'never', 'nevertheless', 'new',\n",
    "    'next', 'no', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'nor', 'not', 'now', 'NULL', 'of', 'off', 'often', 'on',\n",
    "    'once', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'per',\n",
    "    'perhaps', 'rather', 're', 'said', 'same', 'say', 'seem', 'seemed', 'seeming', 'seems', 'several', 'she', 'should',\n",
    "    'shouldn', 'shouldn\\'t', 'since', 'so', 'some', 'still', 'stop', 'such', 'taking', 'ten', 'than', 'that', 'the', 'their',\n",
    "    'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these',\n",
    "    'they', 'this', 'those', 'though', 'thousand', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'toward',\n",
    "    'towards', 'under', 'unless', 'unlike', 'unlikely', 'until', 'up', 'upon', 'us', 'use', 'used', 'using', 've', 'very',\n",
    "    'via', 'was', 'wasn', 'we', 'well', 'were', 'weren', 'weren\\'t', 'what', 'whatever', 'when', 'whence', 'whenever', 'where',\n",
    "    'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who',\n",
    "    'whoever', 'whole', 'whom', 'whomever', 'whose', 'why', 'will', 'with', 'within', 'without', 'won', 'would', 'wouldn',\n",
    "    'wouldn\\'t', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves'\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4a17e9a",
   "metadata": {
    "id": "d4a17e9a"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text, custom_stopwords):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.lower() not in custom_stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "df['Text'] = df['Text'].apply(lambda x: remove_stopwords(x, custom_stopwords))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5f4af8",
   "metadata": {
    "id": "1b5f4af8"
   },
   "source": [
    "To avoid biasness let's suffle the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1249a160",
   "metadata": {
    "id": "1249a160"
   },
   "outputs": [],
   "source": [
    "final_df = df.sample(frac=1, random_state=4525).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f8c227",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "44f8c227",
    "outputId": "64823ffd-8ceb-4cd6-d19c-fca0e9552532"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politics</td>\n",
       "      <td>Protect whistleblowers TUC says government cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Boeing unveils 777 aircraft aircraft firm Boei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>AstraZeneca hit drug failure Shares AngloSwedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tech</td>\n",
       "      <td>Blogger grounded airline airline attendant fig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Fiat chief takes steering wheel chief executiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>sport</td>\n",
       "      <td>Isinbayeva heads Birmingham Olympic pole vault...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>politics</td>\n",
       "      <td>Row police power CSOs Police Federation strong...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>sport</td>\n",
       "      <td>Downing injury mars Uefa victory Middlesbrough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>sport</td>\n",
       "      <td>Houllier praises Benitez regime Former Liverpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Uganda bans Vagina Monologues Ugandas authorit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Category                                               Text\n",
       "0          politics  Protect whistleblowers TUC says government cha...\n",
       "1          business  Boeing unveils 777 aircraft aircraft firm Boei...\n",
       "2          business  AstraZeneca hit drug failure Shares AngloSwedi...\n",
       "3              tech  Blogger grounded airline airline attendant fig...\n",
       "4          business  Fiat chief takes steering wheel chief executiv...\n",
       "...             ...                                                ...\n",
       "2220          sport  Isinbayeva heads Birmingham Olympic pole vault...\n",
       "2221       politics  Row police power CSOs Police Federation strong...\n",
       "2222          sport  Downing injury mars Uefa victory Middlesbrough...\n",
       "2223          sport  Houllier praises Benitez regime Former Liverpo...\n",
       "2224  entertainment  Uganda bans Vagina Monologues Ugandas authorit...\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e22ac22",
   "metadata": {
    "id": "0e22ac22"
   },
   "source": [
    "Let's label the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dc499bf",
   "metadata": {
    "id": "2dc499bf"
   },
   "outputs": [],
   "source": [
    "category_to_label = {\n",
    "    \"business\": 0,\n",
    "    \"entertainment\": 1,\n",
    "    \"politics\": 2,\n",
    "    \"sport\": 3,\n",
    "    \"tech\": 4\n",
    "}\n",
    "\n",
    "df['Label'] = df['Category'].map(category_to_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a528e31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "8a528e31",
    "outputId": "08e636a4-719d-4656-99c8-e1018cbe2973"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit Quarterly pr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains Greenspan speech dollar hit highe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim owners embat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BAs profits British Airwa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Pernod takeover talk lifts Domecq Shares UK dr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>tech</td>\n",
       "      <td>BT program beat dialler scams BT introducing t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>tech</td>\n",
       "      <td>Spam emails tempt net shoppers Computer users ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>tech</td>\n",
       "      <td>careful code European directive put software w...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>tech</td>\n",
       "      <td>cyber security chief resigns man making sure c...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>tech</td>\n",
       "      <td>Losing online gaming Online role playing games...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Category                                               Text  Label\n",
       "0     business  Ad sales boost Time Warner profit Quarterly pr...      0\n",
       "1     business  Dollar gains Greenspan speech dollar hit highe...      0\n",
       "2     business  Yukos unit buyer faces loan claim owners embat...      0\n",
       "3     business  High fuel prices hit BAs profits British Airwa...      0\n",
       "4     business  Pernod takeover talk lifts Domecq Shares UK dr...      0\n",
       "...        ...                                                ...    ...\n",
       "2220      tech  BT program beat dialler scams BT introducing t...      4\n",
       "2221      tech  Spam emails tempt net shoppers Computer users ...      4\n",
       "2222      tech  careful code European directive put software w...      4\n",
       "2223      tech  cyber security chief resigns man making sure c...      4\n",
       "2224      tech  Losing online gaming Online role playing games...      4\n",
       "\n",
       "[2225 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce374f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "Bm_bAMQqQKkA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bm_bAMQqQKkA",
    "outputId": "38529248-8554-4a2f-9400-ffecbfada48b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Average Training Loss: 0.6189186746653702\n",
      "Epoch 2/3, Average Training Loss: 0.10231481795199215\n",
      "Epoch 3/3, Average Training Loss: 0.04708199738524854\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Warning: Loss not available for this batch.\n",
      "Validation Loss: 0.0\n",
      "Validation Accuracy: 0.9707865168539326\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
    "\n",
    "texts = df['Text'].values\n",
    "labels = df['Label'].values\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "tokenized_texts = [tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt') for text in texts]\n",
    "\n",
    "input_ids = torch.cat([t['input_ids'] for t in tokenized_texts])\n",
    "attention_mask = torch.cat([t['attention_mask'] for t in tokenized_texts])\n",
    "\n",
    "dataset = TensorDataset(input_ids, attention_mask, torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * 3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/3, Average Training Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "model.eval()\n",
    "val_loss, correct, total = 0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        if 'loss' in outputs:\n",
    "            loss = outputs.loss\n",
    "            val_loss += loss\n",
    "        else:\n",
    "            print(\"Warning: Loss not available.\")\n",
    "\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predicted_labels == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "print(f\"Validation Loss: {val_loss / len(val_dataloader)}\")\n",
    "print(f\"Validation Accuracy: {correct / total}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inb0x9G2f5ZN",
   "metadata": {
    "id": "inb0x9G2f5ZN"
   },
   "source": [
    "Let's Test the above model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "w1nfkCgidNBK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1nfkCgidNBK",
    "outputId": "91be2c66-639f-487c-88f1-265d8c938a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted category for the given text is: politics\n"
     ]
    }
   ],
   "source": [
    "text_to_predict = \"There have been a number of rows about what should and shouldn't be available to this inquiry. The UK government has been criticised before for not wanting to hand over unredacted WhatsApp messages (it eventually did after a court ruling). The Scottish government is involved in such a row just now - with senior people accused of deleting messages. As a former top official - who knows how Whitehall works better than most - it's significant that Helen MacNamara was so critical of the Cabinet Office. She says it was 'extraordinarily difficult' to get 'basic pieces of information'. She also reveals her government phone was deleted. There will be questions for the government about why.\"\n",
    "\n",
    "input_text = tokenizer(text_to_predict, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "input_text = {key: val.to(device) for key, val in input_text.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**input_text)\n",
    "    predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "labels = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "predicted_category = labels[predicted_label]\n",
    "\n",
    "print(f\"The predicted category for the given text is: {predicted_category}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t4C5PVnzf-Zd",
   "metadata": {
    "id": "t4C5PVnzf-Zd"
   },
   "source": [
    "It is working amazingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s2831R3faqV1",
   "metadata": {
    "id": "s2831R3faqV1"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "sentences = [text.split() for text in train_df['Text']]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "def text_to_vector(text):\n",
    "    tokens = text.split()\n",
    "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "    if vectors:\n",
    "        return sum(vectors) / len(vectors)\n",
    "    else:\n",
    "        return [0] * w2v_model.vector_size\n",
    "\n",
    "train_vectors = [text_to_vector(text) for text in train_df['Text']]\n",
    "test_vectors = [text_to_vector(text) for text in test_df['Text']]\n",
    "\n",
    "clf = XGBClassifier()\n",
    "clf.fit(train_vectors, train_df['Label'])\n",
    "\n",
    "report = classification_report(test_df['Label'], clf.predict(test_vectors), target_names=['business', 'entertainment', 'politics', 'sport', 'tech'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6dTbOAzwaqkV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dTbOAzwaqkV",
    "outputId": "b2dec64c-795f-4aa0-b27d-218720efa981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.91      0.94      0.92       115\n",
      "entertainment       0.86      0.82      0.84        72\n",
      "     politics       0.87      0.88      0.88        76\n",
      "        sport       0.89      0.89      0.89       102\n",
      "         tech       0.88      0.86      0.87        80\n",
      "\n",
      "     accuracy                           0.89       445\n",
      "    macro avg       0.88      0.88      0.88       445\n",
      " weighted avg       0.89      0.89      0.89       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aahNwg6WaqnN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aahNwg6WaqnN",
    "outputId": "32ff430e-9a49-4c73-cb3f-37099102d117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted category for the text is: politics\n"
     ]
    }
   ],
   "source": [
    "text_to_predict = \"There have been a number of rows about what should and shouldn't be available to this inquiry. The UK government has been criticised before for not wanting to hand over unredacted WhatsApp messages (it eventually did after a court ruling). The Scottish government is involved in such a row just now - with senior people accused of deleting messages. As a former top official - who knows how Whitehall works better than most - it's significant that Helen MacNamara was so critical of the Cabinet Office. She says it was 'extraordinarily difficult' to get 'basic pieces of information'. She also reveals her government phone was deleted. There will be questions for the government about why.\"\n",
    "\n",
    "cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text_to_predict)\n",
    "filtered_text = remove_stopwords(cleaned_text, custom_stopwords)\n",
    "vector = text_to_vector(filtered_text)\n",
    "\n",
    "predicted_label = clf.predict([vector])[0]\n",
    "\n",
    "label_to_category = {0: 'business', 1: 'entertainment', 2: 'politics', 3: 'sport', 4: 'tech'}\n",
    "predicted_category = label_to_category[predicted_label]\n",
    "\n",
    "print(f\"The predicted category for the text is: {predicted_category}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TOQ7EdGdgYwt",
   "metadata": {
    "id": "TOQ7EdGdgYwt"
   },
   "source": [
    "It is also working fine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "C-qcsDGugdej",
   "metadata": {
    "id": "C-qcsDGugdej"
   },
   "source": [
    "Comparing Word2Vec and BERT:\n",
    "\n",
    "Word2Vec Results:\n",
    "- Word2Vec does pretty well with an 87% accuracy.\n",
    "- It's like a language detective, good at figuring out word meanings.\n",
    "- Not bad, and it doesn't need tons of computer power.\n",
    "\n",
    "BERT Results:\n",
    "- BERT is the star here, with a whopping 98.43% accuracy.\n",
    "- It's like a language genius, understanding the whole story, not just words.\n",
    "- Almost like having a language expert on your team.\n",
    "\n",
    "Clearly, BERT is the winner\n",
    "\n",
    "- The dataset has a mix of news articles from different categories.\n",
    "- These articles come in various writing styles and flavors.\n",
    "\n",
    "\n",
    "1. Try Different BERTs: We could test out other BERT models like RoBERTa or XLNet; they might do even better.\n",
    "2. Team Up the Models:Combining Word2Vec and BERT cleverly could help, especially if we have lots of data.\n",
    "3. Fine-tune the Models: Tweaking the model settings might reveal hidden talents.\n",
    "4. More Data, Please: Adding more text data would surely pump up our models.\n",
    "\n",
    "In simple terms, BERT is the genius here when it comes to understanding language, and it leaves Word2Vec in the dust. To make it even better, we can train BERT more on our data, clean up the text, or try other smart models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ORzBImLGaqpl",
   "metadata": {
    "id": "ORzBImLGaqpl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L54RdjH8aqr9",
   "metadata": {
    "id": "L54RdjH8aqr9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
